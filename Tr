from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments
from datasets import Dataset
import numpy as np
import torch

# === Split your data into train and test ===
train_df, test_df = train_test_split(data, train_size=750, random_state=42)

# === Prepare tokenizer and model ===
baseline_ckpt = "distilbert-base-uncased"
baseline_tokenizer = AutoTokenizer.from_pretrained(baseline_ckpt)
num_labels = len(id_to_label)
baseline_model = AutoModelForTokenClassification.from_pretrained(baseline_ckpt, num_labels=num_labels)

# === Convert word labels to token labels using the new tokenizer ===
def tokenize_and_align_labels(examples):
    tokenized_inputs = baseline_tokenizer(examples["messages"], truncation=True, padding="max_length", max_length=64, is_split_into_words=False)
    all_labels = []
    for i, labels in enumerate(examples["labels"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        word_labels = labels
        aligned_labels = []
        prev_wid = None
        for wid in word_ids:
            if wid is None:
                aligned_labels.append(-100)
            elif wid != prev_wid:
                aligned_labels.append(label_to_id.get(word_labels[wid], label_to_id["O"]))
            else:
                label = word_labels[wid]
                aligned_labels.append(label_to_id.get(label.replace("B_", "I_"), label_to_id["O"]))
            prev_wid = wid
        all_labels.append(aligned_labels)
    tokenized_inputs["labels"] = all_labels
    return tokenized_inputs

# === Prepare Dataset objects ===
train_ds = Dataset.from_pandas(train_df)
test_ds = Dataset.from_pandas(test_df)
train_tokenized = train_ds.map(tokenize_and_align_labels, batched=True)
test_tokenized = test_ds.map(tokenize_and_align_labels, batched=True)

# === Define metrics ===
from datasets import load_metric
metric = load_metric("seqeval")

def compute_metrics(p):
    preds = np.argmax(p.predictions, axis=2)
    true_labels = p.label_ids
    pred_labels = [
        [id_to_label.get(p, "O") for (p, l) in zip(pred, label) if l != -100]
        for pred, label in zip(preds, true_labels)
    ]
    true = [
        [id_to_label.get(l, "O") for (p, l) in zip(pred, label) if l != -100]
        for pred, label in zip(preds, true_labels)
    ]
    results = metric.compute(predictions=pred_labels, references=true)
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"]
    }

# === Define training arguments ===
args = TrainingArguments(
    output_dir="./baseline_ner",
    evaluation_strategy="epoch",
    logging_dir="./logs",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
)

# === Train baseline model ===
trainer = Trainer(
    model=baseline_model,
    args=args,
    train_dataset=train_tokenized,
    eval_dataset=test_tokenized,
    tokenizer=baseline_tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()

# === Evaluate ===
results = trainer.evaluate()
print("Baseline model performance:")
print(results)
