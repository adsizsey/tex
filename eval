import torch
import random
import re
import pandas as pd
from sklearn.metrics import precision_recall_fscore_support, accuracy_score
import matplotlib.pyplot as plt

# --- Utilities ---
def is_number(token):
    return bool(re.match(r"^\d+(\.\d+)?$", token))

def apply_perturbation_strategy(word, strategy):
    if strategy == "misspell" and len(word) > 3:
        chars = list(word)
        idx = random.randint(1, len(chars) - 2)
        chars[idx], chars[idx+1] = chars[idx+1], chars[idx]
        return "".join(chars)
    elif strategy == "insert":
        return word + "x"
    elif strategy == "combined":
        return apply_perturbation_strategy(apply_perturbation_strategy(word, "misspell"), "insert")
    else:
        return word

# --- Perturb + Re-encode and Trace ---
def trace_prediction_examples(dataset, model, tokenizer, id_to_label, label_to_id,
                              strategy="none", target="all", n=3):
    from IPython.display import display
    model.eval()

    for i in range(n):
        sample = dataset[i]
        words = sample["text"].split()
        label_ids = sample["labels"]

        # Map IDs to string labels, for tracing
        label_strs = [id_to_label[l] if l != -100 else "O" for l in label_ids]

        # Apply word-level perturbation
        perturbed_words = []
        perturbed_labels = []
        for word, label in zip(words, label_strs):
            apply = (target == "all" or
                     ("TICKER" in label and target == "ticker") or
                     (is_number(word) and target == "number"))
            w = apply_perturbation_strategy(word, strategy) if apply else word
            perturbed_words.append(w)
            perturbed_labels.append(label)

        # Tokenize original and perturbed words
        def tokenize_and_align(words, labels):
            encoding = tokenizer(words, is_split_into_words=True, return_offsets_mapping=True,
                                 padding="max_length", truncation=True, return_tensors="pt")
            word_ids = encoding.word_ids()
            aligned = []
            for wi in word_ids:
                if wi is None:
                    aligned.append("O")
                else:
                    aligned.append(labels[wi] if wi < len(labels) else "O")
            return encoding, aligned, word_ids

        enc_orig, aligned_labels, word_ids = tokenize_and_align(words, label_strs)
        enc_pert, aligned_pert_labels, _ = tokenize_and_align(perturbed_words, perturbed_labels)

        input_orig = {k: v.to(model.device) for k, v in enc_orig.items() if k != "offset_mapping"}
        input_pert = {k: v.to(model.device) for k, v in enc_pert.items() if k != "offset_mapping"}

        with torch.no_grad():
            pred_orig = torch.argmax(model(**input_orig).logits, dim=-1)[0].cpu().tolist()
            pred_pert = torch.argmax(model(**input_pert).logits, dim=-1)[0].cpu().tolist()

        tokens = tokenizer.convert_ids_to_tokens(enc_orig["input_ids"][0])
        rows = []
        for j, tok in enumerate(tokens):
            if tok in ["[CLS]", "[SEP]", "[PAD]"]:
                continue
            tl = aligned_labels[j] if j < len(aligned_labels) else "O"
            pl = id_to_label.get(pred_orig[j], "?")
            ppl = id_to_label.get(pred_pert[j], "?")
            rows.append({
                "Idx": j,
                "Token": tok,
                "True Label": tl,
                "Pred Label": pl,
                "Pert Label": ppl
            })

        print(f"\n=== Sample {i} (strategy: {strategy}) ===")
        print("Original Text: ", " ".join(words))
        print("Perturbed Text:", " ".join(perturbed_words))
        display(pd.DataFrame(rows))
