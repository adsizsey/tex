import torch
import random
import re
import pandas as pd
from sklearn.metrics import precision_recall_fscore_support, accuracy_score
import matplotlib.pyplot as plt

# --- Utilities ---
def is_number(token):
    return bool(re.match(r"^\d+(\.\d+)?$", token))

def apply_perturbation_strategy(word, strategy):
    if strategy == "misspell" and len(word) > 3:
        chars = list(word)
        idx = random.randint(1, len(chars) - 2)
        chars[idx], chars[idx+1] = chars[idx+1], chars[idx]
        return "".join(chars)
    elif strategy == "insert":
        return word + "x"
    elif strategy == "combined":
        return apply_perturbation_strategy(apply_perturbation_strategy(word, "misspell"), "insert")
    else:
        return word

# --- Advanced Perturbation + Token-level Label Alignment ---
def perturb_and_tokenize(words, labels, tokenizer, label_to_id, strategy="misspell", target="all"):
    perturbed_words = []
    perturbed_labels = []

    for word, label in zip(words, labels):
        should_perturb = (
            target == "all" or
            (target == "ticker" and "TICKER" in label) or
            (target == "number" and is_number(word))
        )

        new_word = apply_perturbation_strategy(word, strategy) if should_perturb else word
        tokens = tokenizer.tokenize(new_word)

        if not tokens:
            continue  # skip empty tokens

        # assign correct BIO tag propagation
        label_ids = [label_to_id[label]]
        for _ in tokens[1:]:
            if label.startswith("B_"):
                label_ids.append(label_to_id.get("I_" + label[2:], label_to_id["O"]))
            else:
                label_ids.append(label_to_id.get(label, label_to_id["O"]))

        perturbed_words.extend(tokens)
        perturbed_labels.extend(label_ids)

    encoding = tokenizer(perturbed_words, is_split_into_words=False, return_tensors="pt",
                         padding="max_length", truncation=True, max_length=64)
    word_ids = encoding.word_ids()

    aligned_labels = []
    token_cursor = 0
    for word_idx in word_ids:
        if word_idx is None:
            aligned_labels.append(-100)
        else:
            if token_cursor < len(perturbed_labels):
                aligned_labels.append(perturbed_labels[token_cursor])
                token_cursor += 1
            else:
                aligned_labels.append(label_to_id["O"])

    return encoding, aligned_labels, perturbed_words

# --- Trace Predictions ---
def trace_prediction_examples(dataset, model, tokenizer, id_to_label, label_to_id, strategy="none", target="all", n=3):
    from IPython.display import display
    model.eval()

    for i in range(n):
        sample = dataset[i]
        words = sample["text"].split()
        label_ids = sample["labels"]
        label_strs = [id_to_label.get(l, "O") if l != -100 else "O" for l in label_ids]

        encoding, aligned, tokens = perturb_and_tokenize(words, label_strs, tokenizer, label_to_id, strategy, target)
        input_tensor = {k: v.to(model.device) for k, v in encoding.items()}

        with torch.no_grad():
            logits = model(**input_tensor).logits.squeeze()
            predictions = torch.argmax(logits, dim=-1).cpu().tolist()

        decoded_tokens = tokenizer.convert_ids_to_tokens(encoding["input_ids"][0])

        result_rows = []
        for idx, (tok, true_id, pred_id) in enumerate(zip(decoded_tokens, aligned, predictions)):
            if tok in ["[CLS]", "[SEP]", "[PAD]"] or true_id == -100:
                continue
            result_rows.append({
                "Idx": idx,
                "Token": tok,
                "True Label": id_to_label.get(true_id, "?"),
                "Pred Label": id_to_label.get(pred_id, "?")
            })

        print(f"\n=== Sample {i} (strategy: {strategy}) ===")
        display(pd.DataFrame(result_rows))
