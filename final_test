import random
from typing import List, Tuple, Optional
import numpy as np
import pandas as pd
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import matplotlib.pyplot as plt
from tqdm import tqdm
import torch

# --- Perturbation Utilities ---

def perturb_word(word: str, strategy: str) -> str:
    if strategy == "misspell" and len(word) > 1:
        i = random.randint(0, len(word) - 2)
        return word[:i] + word[i+1] + word[i] + word[i+2:]
    elif strategy == "insert" and len(word) > 0:
        i = random.randint(0, len(word))
        return word[:i] + random.choice("xyz") + word[i:]
    return word

def perturb_text(text: str, strategy: str, target: str) -> str:
    words = text.split()
    perturbed = []
    for word in words:
        is_ticker = word.isalpha() and word.isupper()
        is_number = any(c.isdigit() for c in word)
        if (target == "ticker" and is_ticker) or (target == "number" and is_number) or target == "all":
            perturbed.append(perturb_word(word, strategy))
        else:
            perturbed.append(word)
    return " ".join(perturbed)

# --- Tokenization & Label Alignment ---

def tokenize_and_align_labels(tokenizer, text: str, word_labels: List[str], label_to_id: dict):
    tokenized = tokenizer(text, return_offsets_mapping=True, return_tensors='pt', truncation=True, padding='max_length', max_length=64)
    word_ids = tokenized.word_ids()
    aligned = []
    for wid in word_ids:
        if wid is None:
            aligned.append(-100)
        else:
            label = word_labels[wid] if wid < len(word_labels) else 'O'
            aligned.append(label_to_id.get(label, label_to_id['O']))
    return tokenized, aligned

# --- Evaluation ---

def evaluate_case(model, tokenizer, text: str, word_labels: List[str], label_to_id: dict, id_to_label: dict):
    tokenized, aligned_labels = tokenize_and_align_labels(tokenizer, text, word_labels, label_to_id)
    input_ids = tokenized['input_ids'].to(model.device)
    attention_mask = tokenized['attention_mask'].to(model.device)
    with torch.no_grad():
        logits = model(input_ids=input_ids, attention_mask=attention_mask).logits
    pred_ids = torch.argmax(logits, dim=-1).squeeze().tolist()
    pred_labels, true_labels = [], []
    for p, t in zip(pred_ids, aligned_labels):
        if t != -100:
            pred_labels.append(p)
            true_labels.append(t)
    pred_names = [id_to_label[i] for i in pred_labels]
    true_names = [id_to_label[i] for i in true_labels]
    return {
        "accuracy": accuracy_score(true_names, pred_names),
        "precision": precision_score(true_names, pred_names, average="weighted", zero_division=0),
        "recall": recall_score(true_names, pred_names, average="weighted", zero_division=0),
        "f1": f1_score(true_names, pred_names, average="weighted", zero_division=0),
    }

# --- Experiment Runner ---

def run_experiments(dataset, model, tokenizer, id_to_label, label_to_id, strategies, targets, n_samples=200):
    records = []
    for strategy in strategies:
        for target in targets:
            key = f"{strategy}_{target}"
            metrics_list = []
            for i in tqdm(range(min(n_samples, len(dataset)))):
                text = dataset[i]['text']
                
                # Developer-style word label reconstruction
                tokenized = tokenizer(text)
                word_ids = tokenized.word_ids()
                token_labels = dataset[i]['labels']
                max_word_id = max(wid for wid in word_ids if wid is not None)
                word_labels = ['O'] * (max_word_id + 1)
                for lid, wid in zip(token_labels, word_ids):
                    if wid is None or lid == -100:
                        continue
                    label_str = id_to_label[lid]
                    if word_labels[wid] == 'O' or label_str.startswith('B_'):
                        word_labels[wid] = label_str

                if strategy != "none":
                    text = perturb_text(text, strategy, target)
                metrics = evaluate_case(model, tokenizer, text, word_labels, label_to_id, id_to_label)
                metrics_list.append(metrics)
            avg_metrics = {k: np.mean([m[k] for m in metrics_list]) for k in metrics_list[0]}
            avg_metrics['strategy'] = key
            records.append(avg_metrics)
    return pd.DataFrame(records)

# --- Plotting ---

def plot_results(df: pd.DataFrame):
    df.set_index("strategy")[["accuracy", "precision", "recall", "f1"]].plot(kind="bar", figsize=(14,6), title="Performance under different perturbation strategies")
    plt.ylabel("Score")
    plt.ylim(0, 1)
    plt.grid(True)
    plt.tight_layout()
    plt.show()
