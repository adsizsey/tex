import torch
import pandas as pd
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def realistic_misspell(token):
    if token.isalpha() and len(token) > 3:
        return token[:1] + token[2] + token[1] + token[3:]
    return token

def apply_perturbation(words, labels, perturbation_type):
    perturbed = []
    for word, label in zip(words, labels):
        if perturbation_type == "Misspelling" and "TICKER" in label:
            perturbed.append(realistic_misspell(word))
        else:
            perturbed.append(word)
    return perturbed, labels

def tokenize_and_align_labels(words, labels, tokenizer, label_to_id):
    enc = tokenizer(words, is_split_into_words=True, return_tensors="pt", padding="max_length", truncation=True, max_length=64)
    word_ids = enc.word_ids()
    aligned = []
    prev_wid = None
    for wid in word_ids:
        if wid is None:
            aligned.append(-100)
        elif wid != prev_wid:
            aligned.append(label_to_id[labels[wid]])
        else:
            lbl = label_to_id[labels[wid]]
            aligned.append(lbl if id_to_label[lbl].startswith("I_") else lbl)
        prev_wid = wid
    return enc, aligned

def evaluate_robustness(dataset, model, tokenizer, id_to_label, label_to_id, perturbation_type="None", n=5, flag_mismatches=True):
    model.eval()
    trace_rows = []
    all_preds = []
    all_trues = []

    for i in range(min(n, len(dataset))):
        sample = dataset[i]
        words = sample["text"].split()
        labels = [id_to_label[l] for l in sample["labels"]]

        perturbed_words, perturbed_labels = apply_perturbation(words, labels, perturbation_type)

        enc_orig, aligned_orig = tokenize_and_align_labels(words, labels, tokenizer, label_to_id)
        enc_pert, aligned_pert = tokenize_and_align_labels(perturbed_words, perturbed_labels, tokenizer, label_to_id)

        with torch.no_grad():
            pred_orig = model(**enc_orig).logits.argmax(-1).squeeze().tolist()
            pred_pert = model(**enc_pert).logits.argmax(-1).squeeze().tolist()

        toks_orig = tokenizer.convert_ids_to_tokens(enc_orig["input_ids"].squeeze())
        toks_pert = tokenizer.convert_ids_to_tokens(enc_pert["input_ids"].squeeze())

        mask = [l != -100 for l in aligned_orig]
        true_flat = [l for l in aligned_orig if l != -100]
        pred_flat = [p for p, m in zip(pred_pert, mask) if m]
        all_trues.extend(true_flat)
        all_preds.extend(pred_flat)

        mismatch_flags = [("❌" if p != t else "✅") if m else "" for p, t, m in zip(pred_pert, aligned_orig, mask)]

        trace_rows.append({
            "Sample Index": i,
            "Perturbation": perturbation_type,
            "Original Text": sample["text"],
            "Perturbed Text": " ".join(perturbed_words),
            "Original Tokens": list(toks_orig),
            "Perturbed Tokens": list(toks_pert),
            "True Labels": [id_to_label.get(l, "O") if l != -100 else "O" for l in aligned_orig],
            "Pred Orig": [id_to_label.get(p, "O") for p in pred_orig],
            "Pred Pert": [id_to_label.get(p, "O") for p in pred_pert],
            "Mismatch": mismatch_flags if flag_mismatches else None
        })

    precision, recall, f1, _ = precision_recall_fscore_support(all_trues, all_preds, average="weighted", zero_division=0)
    accuracy = accuracy_score(all_trues, all_preds)

    metrics = {
        "Accuracy": round(accuracy, 4),
        "Precision": round(precision, 4),
        "Recall": round(recall, 4),
        "F1 Score": round(f1, 4)
    }

    return pd.DataFrame(trace_rows), metrics


df_trace, metrics = evaluate_robustness(
    dataset=dataset,
    model=model,
    tokenizer=tokenizer,
    id_to_label=id_to_label,
    label_to_id=label_to_id,
    perturbation_type="None",  # or "Misspelling"
    n=5
)

print(df_trace.to_string())
print("\nMetrics:", metrics)
