import torch
import numpy as np
from sklearn.metrics import precision_recall_fscore_support, accuracy_score
from tqdm import tqdm
import random

# === Perturbation logic ===
def misspell_token(token):
    if len(token) > 3:
        i = random.randint(1, len(token) - 2)
        return token[:i] + token[i+1] + token[i] + token[i+2:]
    return token

def inject_noise(tokens):
    return tokens[:random.randint(0, len(tokens))] + ["[NOISE]"] + tokens[random.randint(0, len(tokens)):]

def perturb_input_ids(original_input_ids, tokenizer, model, level="moderate"):
    tokens = tokenizer.convert_ids_to_tokens(original_input_ids)

    if level == "mild":
        tokens = [misspell_token(tok) if random.random() < 0.1 else tok for tok in tokens]
    elif level == "moderate":
        tokens = [misspell_token(tok) if random.random() < 0.2 else tok for tok in tokens]
        tokens = inject_noise(tokens)

    encoded = tokenizer(
        tokens,
        is_split_into_words=True,
        return_tensors="pt",
        truncation=True,
        padding="max_length",
        max_length=model.config.max_position_embeddings
    )
    return encoded["input_ids"].squeeze().tolist()

# === Evaluation function ===
def evaluate(dataset, model, tokenizer, perturb=False, perturb_level="moderate", sample_size=200):
    model.eval()
    device = model.device

    all_preds, all_labels = [], []
    subset = dataset.select(range(min(sample_size, len(dataset)))) if sample_size else dataset

    for input_ids, labels in tqdm(zip(subset["input_ids"], subset["labels"]), total=len(subset)):
        original_labels = labels

        if perturb:
            try:
                input_ids = perturb_input_ids(input_ids, tokenizer, model, level=perturb_level)
            except Exception:
                continue

        attention_mask = [1 if tok != tokenizer.pad_token_id else 0 for tok in input_ids]
        encoded = {
            "input_ids": torch.tensor([input_ids]).to(device),
            "attention_mask": torch.tensor([attention_mask]).to(device)
        }

        with torch.no_grad():
            logits = model(**encoded).logits

        preds = torch.argmax(logits, dim=-1).squeeze().tolist()
        preds = preds[:len(original_labels)]

        filtered_preds = [p for p, l in zip(preds, original_labels) if l != -100]
        filtered_labels = [l for l in original_labels if l != -100]

        if filtered_preds and filtered_labels:
            all_preds.extend(filtered_preds)
            all_labels.extend(filtered_labels)

    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average="weighted", zero_division=0)
    acc = accuracy_score(all_labels, all_preds)

    return {
        "accuracy": round(acc, 4),
        "precision": round(precision, 4),
        "recall": round(recall, 4),
        "f1": round(f1, 4)
    }

# === Run both evaluations ===
print("🔍 Evaluating on original dataset...")
metrics_orig = evaluate(dataset, model, tokenizer, perturb=False, sample_size=200)

print("\n💥 Evaluating on perturbed dataset...")
metrics_pert = evaluate(dataset, model, tokenizer, perturb=True, perturb_level="moderate", sample_size=200)

# === Print results ===
print("\n📊 Evaluation Results:")
print(f"Original:  {metrics_orig}")
print(f"Perturbed: {metrics_pert}")
print(f"🔻 F1 Drop: {metrics_orig['f1'] - metrics_pert['f1']:.4f}")
