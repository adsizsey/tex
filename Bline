import os, torch, numpy as np, random, warnings, matplotlib.pyplot as plt
from copy import deepcopy
from datasets import Dataset
from tqdm.auto import tqdm
from transformers import (DistilBertForTokenClassification, DistilBertTokenizerFast,
                          AutoConfig, TrainingArguments, Trainer)
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

warnings.filterwarnings("ignore")

###############################################################################
# 1 ── LOAD LOCAL distilbert_base WITH FRESH CLASSIFICATION HEAD
###############################################################################
base_path = "./distilbert_base"                      # folder in current dir
num_labels = len(label_to_id)

cfg = AutoConfig.from_pretrained(
    base_path,
    num_labels=num_labels,
    id2label=id_to_label,
    label2id=label_to_id
)

baseline_tokenizer = DistilBertTokenizerFast.from_pretrained(base_path)
baseline_model = DistilBertForTokenClassification.from_pretrained(
    base_path,
    config=cfg,
    ignore_mismatched_sizes=True            # fresh head sized to num_labels
)

###############################################################################
# 2 ── WORD-LABEL HELPER (uses production tokenizer)
###############################################################################
def make_word_labels(text, token_labels):
    """Derive one label per word using production tokenizer majority rule."""
    wid_map = tokenizer(text, return_offsets_mapping=True,
                        padding='max_length', max_length=128,
                        truncation=True).word_ids()
    buckets = {}
    for tok_idx, wid in enumerate(wid_map):
        if wid is None or token_labels[tok_idx] == -100: continue
        buckets.setdefault(wid, []).append(token_labels[tok_idx])

    word_labels = []
    for wid in range(max(buckets)+1):
        if wid not in buckets:
            word_labels.append(label_to_id['O'])
        else:                                     # choose first B_ else majority
            lbls = buckets[wid]
            choose = next((l for l in lbls if id_to_label[l].startswith('B_')), lbls[0])
            word_labels.append(choose)
    return word_labels

###############################################################################
# 3 ── ALIGN THOSE WORD LABELS TO DistilBERT TOKENS
###############################################################################
def encode_distil(text, old_token_labels, max_len=128):
    word_labels = make_word_labels(text, old_token_labels)
    enc = baseline_tokenizer(text, truncation=True, padding='max_length',
                             max_length=max_len, return_offsets_mapping=True)
    tok_lab = []
    for wid in enc.word_ids():
        if wid is None:
            tok_lab.append(-100)
        else:
            tok_lab.append(word_labels[wid] if wid < len(word_labels) else label_to_id['O'])
    enc.pop('offset_mapping', None)
    enc['labels'] = tok_lab
    return enc

def convert_dataset(orig_ds):
    rec = {k: [] for k in ["input_ids", "attention_mask", "labels"]}
    for ex in tqdm(orig_ds, desc="Converting"):
        al = encode_distil(ex["text"], ex["labels"])
        for k in rec: rec[k].append(al[k])
    return Dataset.from_dict(rec)

print("🔄  Converting train / test datasets for DistilBERT …")
train_ds_distil = convert_dataset(train_dataset)
test_ds_distil  = convert_dataset(test_dataset)

###############################################################################
# 4 ── TRAINING SETUP (collator keeps CPU tensors → pin_memory safe)
###############################################################################
def cpu_collator(batch):
    return {
        "input_ids":      torch.tensor([b["input_ids"]      for b in batch], dtype=torch.long),
        "attention_mask": torch.tensor([b["attention_mask"] for b in batch], dtype=torch.long),
        "labels":         torch.tensor([b["labels"]         for b in batch], dtype=torch.long),
    }

training_args = TrainingArguments(
    output_dir="./distilbert_baseline_ckpt",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    learning_rate=5e-5,
    weight_decay=0.01,
    logging_steps=200,
    evaluation_strategy="no",
    save_strategy="no"
)

trainer = Trainer(
    model=baseline_model,
    args=training_args,
    train_dataset=train_ds_distil,
    data_collator=cpu_collator
)

print("⚙️  Training DistilBERT baseline …")
trainer.train()

###############################################################################
# 5 ── TOKEN-LEVEL EVALUATION HELPER
###############################################################################
def evaluate(model, ds, tok):
    model.to(device).eval()
    all_p, all_t = [], []
    dl = torch.utils.data.DataLoader(
        ds, batch_size=32, shuffle=False,
        collate_fn=cpu_collator
    )
    with torch.no_grad():
        for batch in dl:
            ids = batch["input_ids"].to(device)
            msk = batch["attention_mask"].to(device)
            true = batch["labels"]                 # CPU
            preds = model(ids, attention_mask=msk).logits.argmax(-1).cpu()
            for pv,tv in zip(preds, true):
                valid = tv != -100
                all_p.extend(pv[valid].tolist())
                all_t.extend(tv[valid].tolist())
    acc = accuracy_score(all_t, all_p)
    prec, rec, f1, _ = precision_recall_fscore_support(all_t, all_p, average='macro', zero_division=0)
    return {"accuracy":acc, "precision":prec, "recall":rec, "f1":f1}

print("🔍  Evaluating production model …")
prod_metrics  = evaluate(model       , test_dataset , tokenizer)
print("🔍  Evaluating DistilBERT baseline …")
base_metrics  = evaluate(baseline_model, test_ds_distil, baseline_tokenizer)

###############################################################################
# 6 ── DISPLAY COMPARISON
###############################################################################
import pandas as pd, pprint
cmp = pd.DataFrame({"Production":prod_metrics, "DistilBaseline":base_metrics}).T
print("\n=== TOKEN-LEVEL METRIC COMPARISON ===")
display(cmp.round(4))
