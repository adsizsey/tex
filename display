import torch
import random
import re
import pandas as pd
from sklearn.metrics import precision_recall_fscore_support, accuracy_score
import matplotlib.pyplot as plt

# --- Utilities ---
def is_number(token):
    return bool(re.match(r"^\d+(\.\d+)?$", token))

def apply_perturbation_strategy(token, strategy):
    if strategy == "misspell" and len(token) > 3:
        chars = list(token)
        idx = random.randint(1, len(chars) - 2)
        chars[idx], chars[idx+1] = chars[idx+1], chars[idx]
        return "".join(chars)
    elif strategy == "insert":
        return f"{token} x"
    elif strategy == "combined":
        return apply_perturbation_strategy(apply_perturbation_strategy(token, "misspell"), "insert")
    else:
        return token

# --- Perturbation + Alignment ---
def advanced_perturb_tokens(words, labels, tokenizer, label_to_id, strategy="misspell", target="all"):
    perturbed_tokens = []
    new_labels = []

    for word, label in zip(words, labels):
        apply = (
            target == "all" or
            (target == "ticker" and "TICKER" in label) or
            (target == "number" and is_number(word))
        )

        if apply:
            perturbed = apply_perturbation_strategy(word, strategy).split()
            perturbed_tokens.extend(perturbed)
            new_labels.extend([label] + ["O"] * (len(perturbed) - 1))
        else:
            perturbed_tokens.append(word)
            new_labels.append(label)

    encoding = tokenizer(perturbed_tokens, is_split_into_words=True, return_tensors="pt",
                         padding="max_length", truncation=True, max_length=64)
    word_ids = encoding.word_ids()

    aligned_labels = []
    prev_word_idx = None
    for word_idx in word_ids:
        if word_idx is None:
            aligned_labels.append(-100)
        else:
            tag = new_labels[word_idx]
            if word_idx != prev_word_idx:
                aligned_labels.append(label_to_id.get(tag, label_to_id["O"]))
            else:
                tag = "I_" + tag[2:] if tag.startswith("B_") else tag
                aligned_labels.append(label_to_id.get(tag, label_to_id["O"]))
        prev_word_idx = word_idx

    return encoding, aligned_labels, perturbed_tokens

# --- Trace Comparison with Logging ---
def trace_prediction_examples(dataset, model, tokenizer, id_to_label, label_to_id, strategy="misspell", target="ticker", n=3):
    from IPython.display import display
    model.eval()
    for i in range(n):
        sample = dataset[i]
        words = sample["text"].split()
        label_ids = sample["labels"]
        labels_str = [id_to_label.get(lid, "O") if lid != -100 else "O" for lid in label_ids]

        encoding, aligned, perturbed_tokens = advanced_perturb_tokens(words, labels_str, tokenizer, label_to_id, strategy, target)
        word_ids = tokenizer(perturbed_tokens, is_split_into_words=True).word_ids()

        # Safely decode tokens from input_ids if available
        try:
            decoded_tokens = tokenizer.convert_ids_to_tokens(encoding["input_ids"][0])
        except:
            decoded_tokens = perturbed_tokens

        print(f"\n=== Sample {i} ===")
        print("Words:", words)
        print("Original Labels:", labels_str)
        print("Tokens:", decoded_tokens)
        print("Word IDs:", word_ids)

        input_tensor = {k: v.to(model.device) for k, v in encoding.items()}
        with torch.no_grad():
            logits = model(**input_tensor).logits.squeeze()
            predictions = torch.argmax(logits, dim=-1).cpu().tolist()

        result_rows = []
        for idx, (tok, true_id, pred_id) in enumerate(zip(decoded_tokens, aligned, predictions)):
            if true_id != -100:
                result_rows.append({
                    "Idx": idx,
                    "Token": tok,
                    "True Label": id_to_label.get(true_id, "?"),
                    "Pred Label": id_to_label.get(pred_id, "?")
                })
        display(pd.DataFrame(result_rows))
