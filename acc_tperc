# Rebuild predictions_all_levels etc. using existing variables
# Assumes: dataset_by_strategy, model, tokenizer, id_to_label, label_to_id are already available from prior code

from tqdm import tqdm

predictions_all_levels = {}
labels_all_levels = {}
word_ids_all_levels = {}
tokens_all_levels = {}

# Reuse helper
def tokenize_and_align_labels(tokenizer, text: str, word_labels: list, label_to_id: dict):
    tokenized = tokenizer(text, return_offsets_mapping=True, return_tensors='pt', truncation=True, padding='max_length', max_length=64)
    word_ids = tokenized.word_ids()
    aligned = []
    for wid in word_ids:
        if wid is None:
            aligned.append(-100)
        else:
            label = word_labels[wid] if wid < len(word_labels) else 'O'
            aligned.append(label_to_id.get(label, label_to_id['O']))
    return tokenized, aligned

# Main loop
for strategy_name, dataset in dataset_by_strategy.items():
    preds, labels, word_ids, tokens = [], [], [], []
    for sample in tqdm(dataset, desc=strategy_name):
        text = sample['text']
        word_labels = sample['word_labels']
        tokenized = tokenizer(text, return_tensors='pt', return_offsets_mapping=True, truncation=True, padding='max_length', max_length=64)
        word_id_list = tokenized.word_ids()
        input_ids = tokenized['input_ids'].to(model.device)
        attention_mask = tokenized['attention_mask'].to(model.device)

        with torch.no_grad():
            output = model(input_ids=input_ids, attention_mask=attention_mask)
        pred_ids = torch.argmax(output.logits, dim=-1).squeeze().tolist()

        _, label_ids = tokenize_and_align_labels(tokenizer, text, word_labels, label_to_id)

        pred_str = [id_to_label[i] for i, l in zip(pred_ids, label_ids) if l != -100]
        true_str = [id_to_label[l] for l in label_ids if l != -100]
        token_words = tokenizer.convert_ids_to_tokens(tokenized['input_ids'].squeeze().tolist())
        word_ids_cleaned = [wid for i, wid in enumerate(word_id_list) if label_ids[i] != -100]

        preds.append(pred_str)
        labels.append(true_str)
        word_ids.append(word_ids_cleaned)
        tokens.append(token_words)

dataset_by_strategy = {}

for strategy in strategies:
    for target in targets:
        key = f"{strategy}_{target}"
        if key not in dataset_by_strategy:
            dataset_by_strategy[key] = []
        for i in range(min(len(dataset), 500)):  # adjust if needed
            dataset_by_strategy[key].append({
                'text': dataset[i]['text'],  # assuming each sample has .text
                'word_labels': dataset[i]['word_labels']  # assuming word-level labels
            })

    predictions_all_levels[strategy_name] = preds
    labels_all_levels[strategy_name] = labels
    word_ids_all_levels[strategy_name] = word_ids
    tokens_all_levels[strategy_name] = tokens
