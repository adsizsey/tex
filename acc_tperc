# Rebuild predictions_all_levels etc. using existing variables
# Assumes: dataset_by_strategy, model, tokenizer, id_to_label, label_to_id are already available from prior code

from tqdm import tqdm

# Construct dataset_by_strategy using previously used perturbation methods
# This assumes: dataset is the original clean dataset, and you used run_experiments()

# First, reconstruct word_labels from existing dataset entries using id_to_label and tokenizer
from tqdm import tqdm

def extract_word_labels(dataset, tokenizer, id_to_label):
    for entry in dataset:
        text = entry['text']
        labels = entry['labels']
        tokenized = tokenizer(text)
        word_ids = tokenized.word_ids()
        max_wid = max(w for w in word_ids if w is not None)
        word_labels = ['O'] * (max_wid + 1)
        for lid, wid in zip(labels, word_ids):
            if wid is None or lid == -100:
                continue
            label_str = id_to_label[lid]
            if word_labels[wid] == 'O' or label_str.startswith('B_'):
                word_labels[wid] = label_str
        entry['word_labels'] = word_labels

extract_word_labels(dataset, tokenizer, id_to_label)

# Rebuild dataset_by_strategy assuming previous perturbations were applied
# and stored in perturbation_datasets or similar structure
# Here we simulate using the same dataset across strategies for demonstration
strategies = ["none", "misspell_all", "insert_all", "misspell_ticker", "insert_number"]

dataset_by_strategy = {}
for strategy in strategies:
    dataset_by_strategy[strategy] = []
    for entry in dataset:
        dataset_by_strategy[strategy].append({
            'text': entry['text'],
            'word_labels': entry['word_labels']
        })



predictions_all_levels = {}
labels_all_levels = {}
word_ids_all_levels = {}
tokens_all_levels = {}

# Reuse helper
def tokenize_and_align_labels(tokenizer, text: str, word_labels: list, label_to_id: dict):
    tokenized = tokenizer(text, return_offsets_mapping=True, return_tensors='pt', truncation=True, padding='max_length', max_length=64)
    word_ids = tokenized.word_ids()
    aligned = []
    for wid in word_ids:
        if wid is None:
            aligned.append(-100)
        else:
            label = word_labels[wid] if wid < len(word_labels) else 'O'
            aligned.append(label_to_id.get(label, label_to_id['O']))
    return tokenized, aligned

# Main loop
for strategy_name, dataset in dataset_by_strategy.items():
    preds, labels, word_ids, tokens = [], [], [], []
    for sample in tqdm(dataset, desc=strategy_name):
        text = sample['text']
        word_labels = sample['word_labels']
        tokenized = tokenizer(text, return_tensors='pt', return_offsets_mapping=True, truncation=True, padding='max_length', max_length=64)
        word_id_list = tokenized.word_ids()
        input_ids = tokenized['input_ids'].to(model.device)
        attention_mask = tokenized['attention_mask'].to(model.device)

        with torch.no_grad():
            output = model(input_ids=input_ids, attention_mask=attention_mask)
        pred_ids = torch.argmax(output.logits, dim=-1).squeeze().tolist()

        _, label_ids = tokenize_and_align_labels(tokenizer, text, word_labels, label_to_id)

        pred_str = [id_to_label[i] for i, l in zip(pred_ids, label_ids) if l != -100]
        true_str = [id_to_label[l] for l in label_ids if l != -100]
        token_words = tokenizer.convert_ids_to_tokens(tokenized['input_ids'].squeeze().tolist())
        word_ids_cleaned = [wid for i, wid in enumerate(word_id_list) if label_ids[i] != -100]

        preds.append(pred_str)
        labels.append(true_str)
        word_ids.append(word_ids_cleaned)
        tokens.append(token_words)

dataset_by_strategy = {}

for strategy in strategies:
    for target in targets:
        key = f"{strategy}_{target}"
        if key not in dataset_by_strategy:
            dataset_by_strategy[key] = []
        for i in range(min(len(dataset), 500)):  # adjust if needed
            dataset_by_strategy[key].append({
                'text': dataset[i]['text'],  # assuming each sample has .text
                'word_labels': dataset[i]['word_labels']  # assuming word-level labels
            })

    predictions_all_levels[strategy_name] = preds
    labels_all_levels[strategy_name] = labels
    word_ids_all_levels[strategy_name] = word_ids
    tokens_all_levels[strategy_name] = tokens
