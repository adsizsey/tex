import random, re, torch, numpy as np, matplotlib.pyplot as plt
from datasets import Dataset
from copy import deepcopy
from tqdm.auto import tqdm
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from collections import defaultdict

###############################################################################
# 1 â”€â”€ PERTURBATION FUNCTIONS
###############################################################################
def p_misspell_tickers(ex):
    w = ex["text"].split()
    for i, lid in enumerate(ex["labels"]):
        if lid != -100 and id_to_label[lid].endswith("Ticker") and len(w[i]) > 2:
            w[i] = w[i][0] + w[i][2:]          # drop 2nd char
            break
    ex["text"] = " ".join(w);  return ex

def p_misspell_numbers(ex):
    w = ex["text"].split()
    for i, tok in enumerate(w):
        if tok.replace('.','',1).isdigit() and len(tok) > 2:
            w[i] = tok[:-1]; break
    ex["text"] = " ".join(w);  return ex

def p_insert_words(ex, ins=("foo","bar","baz")):
    out=[]
    for tok in ex["text"].split():
        out.append(tok)
        if random.random() < .2: out.append(random.choice(ins))
    ex["text"] = " ".join(out); return ex

def p_combined(ex):       # chain all
    return p_insert_words(p_misspell_numbers(p_misspell_tickers(ex)))

strategies = {
    "NoPerturb"      : lambda x: x,
    "MisspellTicker" : p_misspell_tickers,
    "MisspellNumber" : p_misspell_numbers,
    "InsertWords"    : p_insert_words,
    "CombinedAll"    : p_combined
}

###############################################################################
# 2 â”€â”€ TOKENIZE & ALIGN  (CLS/SEP/PAD -100, new tokens â†’ 'O')
###############################################################################
def tok_align(text:str, old_tok_labels:list[int], max_len:int=128):
    enc = tokenizer(text, truncation=True, padding='max_length',
                    max_length=max_len, return_tensors=None)
    core_old = old_tok_labels[1:-1]                  # keep entire 126-token core inc. pad
    new_lab  = [-100]                                # CLS
    ptr=0
    for _ in enc["input_ids"][1:-1]:                 # core portion
        if ptr < len(core_old):
            new_lab.append(core_old[ptr]); ptr+=1
        else:
            new_lab.append(label_to_id['O'])         # brand-new token from insertion
    new_lab.append(-100)                             # SEP
    enc["labels"] = new_lab
    return enc

###############################################################################
# 3 â”€â”€ BUILD DATASET PER STRATEGY
###############################################################################
def build_sets(base_ds):
    out={}
    for name, fn in strategies.items():
        print(f"ðŸ›   Building â†’ {name}")
        if name=="NoPerturb":                        # reuse original untouched dataset
            out[name]=base_ds
            continue
        pert = base_ds.map(lambda ex: fn(deepcopy(ex)))
        cols = {k:[] for k in ["input_ids","attention_mask","labels","text"]}
        for ex in tqdm(pert, desc=name):
            aligned = tok_align(ex["text"], ex["labels"])
            cols["input_ids"].append(aligned["input_ids"])
            cols["attention_mask"].append(aligned["attention_mask"])
            cols["labels"].append(aligned["labels"])
            cols["text"].append(ex["text"])
        out[name] = Dataset.from_dict(cols)
    return out

###############################################################################
# 4 â”€â”€ EVALUATION  (token, word, sample metrics)
###############################################################################
def eval_set(ds):
    model.to(device).eval()
    tok_p, tok_t = [], []
    w_p, w_t     = [], []
    samp_scores  = []

    loader = torch.utils.data.DataLoader(
        ds, batch_size=16, shuffle=False,
        collate_fn=lambda b:{k:[d[k] for d in b] for k in b[0]}
    )
    with torch.no_grad():
        for batch in loader:
            ids = torch.tensor(batch["input_ids"]).to(device)
            msk = torch.tensor(batch["attention_mask"]).to(device)
            true = batch["labels"]                       # list[list[int]]
            text = batch["text"]

            preds = model(ids, attention_mask=msk).logits.argmax(-1).cpu().tolist()

            for pv, tv, txt in zip(preds, true, text):
                idx   = [i for i,l in enumerate(tv) if l!=-100]
                if not idx: continue
                pv_f  = [pv[i] for i in idx]
                tv_f  = [tv[i] for i in idx]
                tok_p.extend(pv_f); tok_t.extend(tv_f)

                # word-level majority vote
                wid_map = tokenizer(txt, return_offsets_mapping=True).word_ids()
                bucket_pred, bucket_true = defaultdict(list), defaultdict(list)
                for i in idx:
                    wid = wid_map[i]
                    if wid is not None:
                        bucket_pred[wid].append(pv[i])
                        bucket_true[wid].append(tv[i])
                for wid in bucket_pred:
                    maj_p = max(set(bucket_pred[wid]), key=bucket_pred[wid].count)
                    maj_t = max(set(bucket_true[wid]), key=bucket_true[wid].count)
                    w_p.append(maj_p); w_t.append(maj_t)

                samp_scores.append(sum(int(a==b) for a,b in zip(pv_f,tv_f))/len(pv_f))

    # metrics helper
    def macro(true, pred):
        acc  = accuracy_score(true, pred) if true else 0.0
        pr, rc, f1,_ = precision_recall_fscore_support(
            true, pred, average='macro', zero_division=0
        ) if true else (0,0,0,0)
        return acc, pr, rc, f1
    tok_acc, tok_pr, tok_rc, tok_f1 = macro(tok_t, tok_p)
    w_acc,  w_pr,  w_rc,  w_f1      = macro(w_t,  w_p)

    pct=lambda th: np.mean([s>=th for s in samp_scores]) if samp_scores else 0.0
    return {
        # token
        "token_acc": tok_acc, "token_prec": tok_pr, "token_rec": tok_rc, "token_f1": tok_f1,
        # word
        "word_acc" : w_acc,  "word_prec" : w_pr,  "word_rec" : w_rc,  "word_f1" : w_f1,
        # sample thresholds
        "sampleâ‰¥50%": pct(.5), "sampleâ‰¥70%": pct(.7),
        "sampleâ‰¥90%": pct(.9), "sample=100%": pct(1.0)
    }

###############################################################################
# 5 â”€â”€ RUN + PLOT
###############################################################################
datasets = build_sets(test_dataset)
results  = {n: eval_set(ds) for n,ds in datasets.items()}

for metric in results[next(iter(results))]:
    plt.figure(figsize=(8,4))
    plt.bar(results.keys(), [results[s][metric] for s in results], color="royalblue")
    plt.title(f"{metric} Across Perturbations"); plt.ylabel("Score")
    plt.ylim(0,1); plt.xticks(rotation=20); plt.grid(axis='y', ls='--', alpha=.5)
    plt.tight_layout(); plt.show()
