import random, re, torch, numpy as np, matplotlib.pyplot as plt
from copy import deepcopy
from datasets import Dataset
from tqdm.auto import tqdm

###############################################################################
# 1 ─────  PERTURBATION FUNCTIONS
###############################################################################
def misspell_tickers(ex):
    words = ex["text"].split()
    for i, lid in enumerate(ex["labels"]):
        if lid == -100:
            continue
        if id_to_label[lid].endswith("Ticker") and len(words[i]) > 2:
            words[i] = words[i][0] + words[i][2:]            # drop 2nd char
            break
    ex["text"] = " ".join(words)
    return ex

def misspell_numbers(ex):
    words = ex["text"].split()
    for i, w in enumerate(words):
        if w.replace(".", "", 1).isdigit() and len(w) > 2:
            words[i] = w[:-1]                                # drop last digit
            break
    ex["text"] = " ".join(words)
    return ex

def insert_words(ex, insertables=("foo", "bar", "baz")):
    ws, out = ex["text"].split(), []
    for w in ws:
        out.append(w)
        if random.random() < 0.20:
            out.append(random.choice(insertables))
    ex["text"] = " ".join(out)
    return ex

def combined_all(ex):
    return insert_words(misspell_numbers(misspell_tickers(ex)))

###############################################################################
# 2 ─────  STRATEGY DICTIONARY
###############################################################################
strategies = {
    "NoPerturb"      : lambda x: x,
    "MisspellTicker" : misspell_tickers,
    "MisspellNumber" : misspell_numbers,
    "InsertWords"    : insert_words,
    "CombinedAll"    : combined_all,
}

###############################################################################
# 3 ─────  TOKENIZE + ALIGN  (-100 for specials / padding)
###############################################################################
def tokenize_align(text:str, word_labels:list[int]):
    tok = tokenizer(
        text, return_offsets_mapping=True,
        padding="max_length", truncation=True, max_length=128
    )
    # map token → word-index
    wids = tok.word_ids()
    tlabs = []
    for wid in wids:
        if wid is None:
            tlabs.append(-100)                       # CLS, SEP, PAD
        else:
            tlabs.append(word_labels[wid] if wid < len(word_labels) else label_to_id["O"])
    tok.pop("offset_mapping")
    tok["labels"] = tlabs
    return tok

###############################################################################
# 4 ─────  BUILD DATASETS FOR EVERY STRATEGY
###############################################################################
dataset_by_strategy = {}

print("\n⌛  Applying perturbations & realigning …")
for sname, fn in strategies.items():
    pert = test_dataset.map(lambda ex: fn(deepcopy(ex)))
    # realign labels sample-by-sample
    toks = {k: [] for k in ["input_ids", "attention_mask", "labels", "text"]}
    for ex in tqdm(pert, desc=f"→ {sname}"):
        aligned = tokenize_align(ex["text"], ex["labels"])
        toks["input_ids"].append(aligned["input_ids"])
        toks["attention_mask"].append(aligned["attention_mask"])
        toks["labels"].append(aligned["labels"])
        toks["text"].append(ex["text"])
    dataset_by_strategy[sname] = Dataset.from_dict(toks)

###############################################################################
# 5 ─────  EVALUATION  (token & sample thresholds)
###############################################################################
def eval_dataset(ds):
    model.to(device); model.eval()
    all_p, all_t, samp_scores = [], [], []

    loader = torch.utils.data.DataLoader(ds, batch_size=16, shuffle=False)
    for batch in loader:
        inp = torch.tensor(batch["input_ids"]).to(device)
        msk = torch.tensor(batch["attention_mask"]).to(device)
        lbl = torch.tensor(batch["labels"])                     # stay on CPU for list ops
        with torch.no_grad():
            logits = model(input_ids=inp, attention_mask=msk).logits
        preds = torch.argmax(logits, -1).cpu()

        for p, t in zip(preds, lbl):
            valid = t != -100
            pv, tv = p[valid].tolist(), t[valid].tolist()
            if not pv: continue
            match = [pi == ti for pi, ti in zip(pv, tv)]
            all_p.extend(pv); all_t.extend(tv)
            samp_scores.append(sum(match)/len(match))

    def thr(x): return float(np.mean([s >= x for s in samp_scores])) if samp_scores else 0.0
    return {
        "token_acc":   float(np.mean([p == t for p, t in zip(all_p, all_t)])) if all_p else 0.0,
        "sample_50":   thr(0.50),
        "sample_70":   thr(0.70),
        "sample_90":   thr(0.90),
        "sample_100":  thr(1.00),
    }

print("\n⚙️  Evaluating each strategy …")
results = {s: eval_dataset(ds) for s, ds in dataset_by_strategy.items()}

###############################################################################
# 6 ─────  PLOT RESULTS
###############################################################################
metrics = list(next(iter(results.values())).keys())
for m in metrics:
    plt.figure(figsize=(7,4))
    plt.bar(results.keys(), [results[s][m] for s in results])
    plt.title(f"{m} across perturbations")
    plt.ylim(0,1)
    plt.xticks(rotation=25)
    plt.grid(axis="y", linestyle="--", alpha=0.5)
    plt.tight_layout()
    plt.show()
