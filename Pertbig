import numpy as np
from sklearn.metrics import precision_recall_fscore_support, accuracy_score
from transformers import AutoTokenizer
from datasets import Dataset
import matplotlib.pyplot as plt
import torch
import re
import random

# Assumed to be already defined and available:
# - model (loaded and on correct device)
# - tokenizer (original one used for training)
# - test_dataset (with keys: input_ids, attention_mask, labels, text)
# - id_to_label, label_to_id

def safe_model_predictions(dataset):
    model.eval()
    preds_all, labels_all = [], []
    for entry in dataset:
        with torch.no_grad():
            input_ids = torch.tensor([entry['input_ids']], dtype=torch.long)
            attn_mask = torch.tensor([entry['attention_mask']], dtype=torch.long)
            logits = model(input_ids=input_ids, attention_mask=attn_mask).logits
            preds = torch.argmax(logits, dim=-1).squeeze().tolist()
        true_labels = entry["labels"]
        # Remove [CLS] and [SEP]
        preds = preds[1:-1]
        true_labels = true_labels[1:-1]
        min_len = min(len(preds), len(true_labels))
        preds_all.extend(preds[:min_len])
        labels_all.extend(true_labels[:min_len])
    return preds_all, labels_all

def compute_metrics(preds, labels):
    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average="macro", zero_division=0)
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "precision": p, "recall": r, "f1": f1}

def misspell_numbers(text):
    return re.sub(r'\d+', lambda m: m.group(0) + random.choice("abc"), text)

def misspell_tickers(text):
    return re.sub(r'\b[A-Z]{2,5}\b', lambda m: m.group(0) + random.choice("xyz"), text)

def insert_words(text):
    words = text.split()
    insert_idx = random.randint(0, len(words))
    insert_word = random.choice(["dummy", "hello", "test"])
    return " ".join(words[:insert_idx] + [insert_word] + words[insert_idx:])

def prepare_dataset(texts):
    out = []
    for txt in texts:
        tok = tokenizer(txt, truncation=True, padding="max_length", max_length=512)
        labels = [label_to_id["O"]] * len(tok["input_ids"])
        out.append({
            "text": txt,
            "input_ids": tok["input_ids"],
            "attention_mask": tok["attention_mask"],
            "labels": labels
        })
    return Dataset.from_list(out)

# Build perturbed datasets
texts = [x['text'] for x in test_dataset]
perturbations = {
    "Original": texts,
    "MisspellTickers": [misspell_tickers(t) for t in texts],
    "MisspellNumbers": [misspell_numbers(t) for t in texts],
    "InsertWords": [insert_words(t) for t in texts],
}

results = {}
for name, pert_texts in perturbations.items():
    pert_dataset = prepare_dataset(pert_texts)
    preds, labels = safe_model_predictions(pert_dataset)
    results[name] = compute_metrics(preds, labels)

# Plot
metric_names = ["accuracy", "precision", "recall", "f1"]
x = np.arange(len(results))
width = 0.2

for i, metric in enumerate(metric_names):
    plt.figure(figsize=(8, 4))
    values = [results[k][metric] for k in results]
    plt.bar(x + i * width, values, width=width, label=metric.capitalize())
    plt.xticks(x + width, list(results.keys()))
    plt.ylabel(metric.capitalize())
    plt.title(f"{metric.capitalize()} Comparison Across Perturbations")
    plt.legend()
    plt.tight_layout()
    plt.show()
