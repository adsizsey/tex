import torch
import pandas as pd
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def evaluate_robustness_fixed_trace(dataset, model, tokenizer, id_to_label, label_to_id, perturbation_type="None", n=5, flag_mismatches=True):

    def realistic_misspell(token):
        if token.isalpha() and len(token) > 3:
            return token[:1] + token[2] + token[1] + token[3:]
        return token

    def apply_perturbation(words, labels, perturbation_type):
        perturbed = []
        for word, label in zip(words, labels):
            if perturbation_type == "Misspelling" and "TICKER" in label:
                perturbed.append(realistic_misspell(word))
            else:
                perturbed.append(word)
        return perturbed, labels

    def tokenize_and_align_labels(words, label_ids, tokenizer, label_to_id, id_to_label):
        # Step 1: Convert label_ids to label strings
        labels = []
        for lid in label_ids:
            if lid == -100:
                labels.append("O")
            elif isinstance(lid, str):
                labels.append(lid)
            else:
                labels.append(id_to_label[lid])

        # Step 2: Tokenize and get word_id map
        encoding = tokenizer(words, is_split_into_words=True, return_tensors="pt", padding="max_length", truncation=True, max_length=64)
        word_ids = encoding.word_ids()

        aligned = []
        aligned_text = []
        prev_word_idx = None

        for word_idx in word_ids:
            if word_idx is None:
                aligned.append(-100)
                aligned_text.append("O")
                continue

            orig_label = labels[word_idx]

            # First token of a word
            if word_idx != prev_word_idx:
                aligned_label = orig_label
            else:
                if orig_label.startswith("B_"):
                    aligned_label = "I_" + orig_label[2:]
                elif orig_label.startswith("I_") or orig_label != "O":
                    aligned_label = orig_label
                else:
                    aligned_label = "O"

            aligned.append(label_to_id.get(aligned_label, label_to_id["O"]))
            aligned_text.append(aligned_label)
            prev_word_idx = word_idx

        return encoding, aligned, aligned_text

    model.eval()
    trace_rows = []
    all_preds = []
    all_trues = []

    for i in range(min(n, len(dataset))):
        sample = dataset[i]
        words = sample["text"].split()
        label_ids = sample["labels"]

        # Encode original
        enc_orig, aligned_orig, aligned_orig_text = tokenize_and_align_labels(words, label_ids, tokenizer, label_to_id, id_to_label)

        # Perturbation
        word_level_labels = [id_to_label[l] if l != -100 else "O" for l in label_ids]
        perturbed_words, _ = apply_perturbation(words, word_level_labels, perturbation_type)

        # Encode perturbed
        enc_pert, aligned_pert, _ = tokenize_and_align_labels(perturbed_words, label_ids, tokenizer, label_to_id, id_to_label)

        enc_orig = {k: v.to(model.device) for k, v in enc_orig.items()}
        enc_pert = {k: v.to(model.device) for k, v in enc_pert.items()}

        with torch.no_grad():
            pred_orig = model(**enc_orig).logits.argmax(-1).squeeze().tolist()
            pred_pert = model(**enc_pert).logits.argmax(-1).squeeze().tolist()

        toks_orig = tokenizer.convert_ids_to_tokens(enc_orig["input_ids"].squeeze())
        toks_pert = tokenizer.convert_ids_to_tokens(enc_pert["input_ids"].squeeze())

        mask = [l != -100 for l in aligned_orig]
        true_flat = [l for l in aligned_orig if l != -100]
        pred_flat = [p for p, m in zip(pred_pert, mask) if m]
        all_trues.extend(true_flat)
        all_preds.extend(pred_flat)

        mismatch_flags = [("❌" if p != t else "✅") if m else "" for p, t, m in zip(pred_pert, aligned_orig, mask)]

        trace_rows.append({
            "Sample Index": i,
            "Perturbation": perturbation_type,
            "Original Text": sample["text"],
            "Perturbed Text": " ".join(perturbed_words),
            "Original Tokens": toks_orig,
            "Perturbed Tokens": toks_pert,
            "True Labels": aligned_orig_text,
            "Pred Orig": [id_to_label.get(p, "O") for p in pred_orig],
            "Pred Pert": [id_to_label.get(p, "O") for p in pred_pert],
            "Mismatch": mismatch_flags if flag_mismatches else None
        })

    precision, recall, f1, _ = precision_recall_fscore_support(all_trues, all_preds, average="weighted", zero_division=0)
    accuracy = accuracy_score(all_trues, all_preds)

    metrics = {
        "Accuracy": round(accuracy, 4),
        "Precision": round(precision, 4),
        "Recall": round(recall, 4),
        "F1 Score": round(f1, 4)
    }

    return pd.DataFrame(trace_rows), metrics
